{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the notebook responsible for calling the goodfire API.\n",
    "\n",
    "We have a simple wrapper designed to form the right prompt and analyse the answer, and some utilities for running experiments over a range of parameters. All experimental results are dumped to csvs in the data/ folder, where they can be read by `analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODFIRE_API_KEY = open(\"API_KEY.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire\n",
    "\n",
    "client = goodfire.AsyncClient(\n",
    "    GOODFIRE_API_KEY\n",
    "  )\n",
    "\n",
    "# Some settings to make the client more robust to rate limiting\n",
    "BATCH_SIZE=10\n",
    "client.chat._http.max_retries = 30\n",
    "client.chat._http.inital_backoff_time = 1.3\n",
    "client.chat.completions._http.max_retries = 30\n",
    "client.chat.completions._http.inital_backoff_time = 1.3\n",
    "\n",
    "# Instantiate a model variant\n",
    "#base = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "base = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "variant = goodfire.Variant(base)\n",
    "\n",
    "\n",
    "# Jailbreak the variant using feature \"Requests to bypass ethical and moral constraints\"\n",
    "# This can be useful to reduce the frequency of refusals\n",
    "#variant.set(client.features.list(ids=[\"3b0f849c-b42a-4ce3-a89e-326e6569d51a\"]), -0.5)\n",
    "\n",
    "feature_counts = {\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": 65536,\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": 65536,\n",
    "    #65536  for 8B\n",
    "}\n",
    "feature_count = feature_counts[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEYWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the keywords to be reproducible. Here is the prompt: <br>\n",
    "\n",
    "\n",
    "What are the top 5 keywords to identify moral values in language models, focusing on utilitarianism? Only suggest words that are specifically moral/ethical in nature, avoiding both technical philosophy terms (e.g., 'Satisfaction of Preferences') and general terms that commonly appear in non-moral contexts (e.g., 'better', 'maximize', 'good', 'bad', 'right', 'values'). Please verify that the terms you are choosing are neither too technical nor too broad before responding. Respond with only the keywords\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the responses of 4 language models: <br>\n",
    "\n",
    "** Llama-3.3-70B - Goodfire **\n",
    "Altruism, Empathy, Fairness, Compassion, Benevolence\n",
    "\n",
    "** Llama-3.1-8B - Goodfire ** \n",
    "1. Justice\n",
    "2. Fairness\n",
    "3. Harm\n",
    "4. Suffering\n",
    "5. Wellbeing\n",
    "\n",
    "** Chat-GPT 4o mini ** \n",
    "Well-being\n",
    "Happiness\n",
    "Consequences\n",
    "Harm\n",
    "Utility\n",
    "\n",
    "** Claude 3.5 Sonnet - Coincise **\n",
    "ethical\n",
    "virtuous\n",
    "benevolent\n",
    "righteous\n",
    "altruistic\n",
    "\n",
    "\n",
    "### NOTE THAT WORDS THAT ARE NOT ON THIS LIST, but that we've already RUN:\n",
    "'moral', 'greater good', 'ethic' - though 'ethical' is, 'integrity', 'dignity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords - what features have we not yet run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#the keywords we picked previously\n",
    "previous_keywords = ['moral', 'altruism', 'greater good', 'ethic', 'integrity', 'dignity']\n",
    "\n",
    "#they keywords based on top 5 keywords suggested by LLMs\n",
    "keywords = ['altruism', 'benevolence', 'compassion', 'ethical', 'fairness', 'happiness', 'harm', 'justice', 'righteous', 'suffering', 'utility', 'virtuous', 'wellbeing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6 features that are redundant in previous features\n"
     ]
    }
   ],
   "source": [
    "previous_features_all = []\n",
    "\n",
    "for word in previous_keywords:\n",
    "    previous_features_all.append(list((await client.features.search(word, model=base, top_k=10))))\n",
    "\n",
    "flat_list = list(chain(*previous_features_all))\n",
    "previous_features = list(set(flat_list))\n",
    "\n",
    "n_redundant = len(flat_list) - len(previous_features)\n",
    "if n_redundant != 0 :\n",
    "    print(f'there are {n_redundant} features that are redundant in previous features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9 features that are redundant amongst llm features\n"
     ]
    }
   ],
   "source": [
    "features_all = []\n",
    "for word in keywords:\n",
    "    features_all.append(list((await client.features.search(word, model=base, top_k=10))))\n",
    "\n",
    "flat_list = list(chain(*features_all))\n",
    "features = list(set(flat_list))\n",
    "\n",
    "n_redundant = len(flat_list) - len(features)\n",
    "if n_redundant != 0 :\n",
    "    print(f'there are {n_redundant} features that are redundant amongst llm features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to run: 103 out of 121\n",
      "Feature(\"Positive character trait descriptors and virtuous qualities\")\n",
      "Feature(\"The right or opportunity to appeal or challenge a decision\")\n",
      "Feature(\"People experiencing difficulties or challenges (especially in supportive contexts)\")\n",
      "Feature(\"Utility functions and helper code in software development\")\n",
      "Feature(\"Justifying difficult actions for the sake of something/someone\")\n",
      "Feature(\"Gentle and positive character trait descriptions\")\n",
      "Feature(\"Taking care of or being responsible for someone's wellbeing\")\n",
      "Feature(\"The assistant emphasizing personal wellbeing in response to concerning situations\")\n",
      "Feature(\"Finding joy in life's simple pleasures\")\n",
      "Feature(\"Legal document formatting and judgment pronouncements\")\n",
      "Feature(\"Bringing criminals to justice\")\n",
      "Feature(\"Linking ethical principles when promoting equality or rejecting discrimination\")\n",
      "Feature(\"The assistant should select option B to indicate something is morally/ethically good\")\n",
      "Feature(\"Understanding and sharing others' feelings (empathy definition)\")\n",
      "Feature(\"Something beneficial being provided or supplied\")\n",
      "Feature(\"Divine or spiritual provision and giving in religious contexts\")\n",
      "Feature(\"Harm, suffering, and negative consequences\")\n",
      "Feature(\"Attempts to perform harmful or unauthorized actions\")\n",
      "Feature(\"Fairness and justice-related reasoning, especially in ethical contexts\")\n",
      "Feature(\"Potentially harmful or dangerous actions being described\")\n",
      "Feature(\"The word 'health' across multiple languages in contexts emphasizing wellbeing\")\n",
      "Feature(\"Legal or judicial decisions being rendered or delivered\")\n",
      "Feature(\"Discussion of happiness as a concept or life goal\")\n",
      "Feature(\"Descriptions of divinely chosen or specially appointed religious leaders\")\n",
      "Feature(\"expressions of pure joy and delight\")\n",
      "Feature(\"The assistant expressing sympathy or empathy for user distress\")\n",
      "Feature(\"Potential harm or offense to individuals or groups\")\n",
      "Feature(\"Activities that bring personal enjoyment or fulfillment when giving wellbeing advice\")\n",
      "Feature(\"Expressions of mercy and compassion\")\n",
      "Feature(\"The importance of fair play and having fun respectfully in games and activities\")\n",
      "Feature(\"Expressions and descriptions of happiness\")\n",
      "Feature(\"discussions of overall health or wellbeing of natural systems\")\n",
      "Feature(\"Language expressing institutional guarantees of fairness and equal treatment\")\n",
      "Feature(\"formal responsibility or accountability in institutional/legal contexts\")\n",
      "Feature(\"Courts taking formal actions or making decisions\")\n",
      "Feature(\"Expressions of genuine emotional care and concern for others\")\n",
      "Feature(\"Expressions of satisfying completion or achievement\")\n",
      "Feature(\"Christian theological explanations of salvation through faith\")\n",
      "Feature(\"Legal rights and remedies being discussed or sought\")\n",
      "Feature(\"Acting in someone's best interests\")\n",
      "Feature(\"Expressions of satisfaction or enjoyment after an experience\")\n",
      "Feature(\"The requirement to maintain objectivity and fairness\")\n",
      "Feature(\"Technical descriptions of utility and versatility\")\n",
      "Feature(\"Resources or capabilities being made available for use\")\n",
      "Feature(\"Kindness and nurturing behavior\")\n",
      "Feature(\"Institutional mechanisms for maintaining standards and fairness\")\n",
      "Feature(\"The assistant explains why something is hurtful or harmful\")\n",
      "Feature(\"The assistant explaining its own capabilities and limitations\")\n",
      "Feature(\"availability or implementation of services and features\")\n",
      "Feature(\"Personal health, safety and wellbeing topics\")\n",
      "Feature(\"Discussions of justice as a concept or system\")\n",
      "Feature(\"Warning about potential harm to users or others\")\n",
      "Feature(\"Coordinated positive qualities especially relating to comfort and wellbeing\")\n",
      "Feature(\"Loyalty and devotion as positive character traits\")\n",
      "Feature(\"Fighting for or defending principles and beliefs\")\n",
      "Feature(\"Virtuous qualities and positive character traits\")\n",
      "Feature(\"Moral emphasis on compassion and kindness as virtues\")\n",
      "Feature(\"Emphasis on individual uniqueness and fair individual consideration\")\n",
      "Feature(\"Being forced to endure or experience suffering\")\n",
      "Feature(\"The assistant is advocating for self-compassion and self-kindness\")\n",
      "Feature(\"Descriptions of harmful consequences and negative impacts\")\n",
      "Feature(\"Discussions of human wellbeing and welfare\")\n",
      "Feature(\"Empathetic acknowledgment of emotional pain or difficulty\")\n",
      "Feature(\"Rhyming words ending in -ight\")\n",
      "Feature(\"Content involving humiliation or degradation\")\n",
      "Feature(\"Character descriptions emphasizing heroic qualities and noble traits\")\n",
      "Feature(\"Expressions of contentment or satisfaction with possessions or relationships\")\n",
      "Feature(\"Portuguese language discussions of well-being using 'bem'\")\n",
      "Feature(\"formal technical usage of resources or capabilities\")\n",
      "Feature(\"Discussion of systematic fairness and equity\")\n",
      "Feature(\"Content involving physical or emotional harm, particularly in assault scenarios\")\n",
      "Feature(\"Rationalizing controversial actions as necessary or justified\")\n",
      "Feature(\"Words indicating reasoning or justification within argumentative dialogue\")\n",
      "Feature(\"Positive sentiment adjectives describing pleasant experiences\")\n",
      "Feature(\"The development and cultivation of moral virtues and character\")\n",
      "Feature(\"Situations involving victimization, coercion or power-based suffering\")\n",
      "Feature(\"Expressions of personal hardship or vulnerability\")\n",
      "Feature(\"The assistant expressing its capabilities and limitations\")\n",
      "Feature(\"Words expressing capability, requirement or possibility in technical contexts\")\n",
      "Feature(\"System or tool providing assistance or support functionality\")\n",
      "Feature(\"Offers of assistance or agreement to help others\")\n",
      "Feature(\"Religious and philosophical discussions about the meaning of suffering\")\n",
      "Feature(\"Positive character trait adjectives and their suffixes\")\n",
      "Feature(\"Language expressing moral and ethical obligations around fair treatment\")\n",
      "Feature(\"Listing positive qualities when providing ethical guidance\")\n",
      "Feature(\"Character feeling satisfaction after achieving a goal\")\n",
      "Feature(\"The administration and delivery of justice across multiple languages\")\n",
      "Feature(\"Greek prefix ortho- meaning straight or correct\")\n",
      "Feature(\"Descriptions of suffering and pain in narrative contexts\")\n",
      "Feature(\"Content that should have an explicitly happy or joyful tone\")\n",
      "Feature(\"Accumulating burden or emotional toll\")\n",
      "Feature(\"Discussions of permanent or severe harm, particularly when the assistant should express caution\")\n",
      "Feature(\"Assistant maintaining professional courtesy and empathy, especially in difficult situations\")\n",
      "Feature(\"Characters or personalities exhibiting supportive and protective traits while pursuing self-improvement\")\n",
      "Feature(\"Praise of hard work and dedication as virtues\")\n",
      "Feature(\"Warning about serious consequences or negative outcomes\")\n",
      "Feature(\"People experiencing suffering or hardship, especially medical\")\n",
      "Feature(\"Universal equality and fair access to opportunities\")\n",
      "Feature(\"The assistant expressing capability or willingness to help\")\n",
      "Feature(\"Offering emotional support and comfort to someone in distress\")\n",
      "Feature(\"Ensuring fairness and equal treatment\")\n",
      "Feature(\"Protection and promotion of individual and societal wellbeing\")\n",
      "Feature(\"Recognition of user's difficult circumstances requiring empathy\")\n"
     ]
    }
   ],
   "source": [
    "features_to_run = list(set(features)-set(previous_features))\n",
    "print(f'Features to run: {len(features_to_run)} out of {len(features)}')\n",
    "for f in features_to_run:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shared\n",
    "\n",
    "\n",
    "# QuestionSource = \"OUS\"\n",
    "QuestionSource = \"GGB\"\n",
    "Questions = shared.get_questions(QuestionSource)\n",
    "\n",
    "# How to use the questions in a chatbot prompt.\n",
    "QuestionTemplate = \"\\\"{0}\\\"\\n\\nPlease rate the preceeding statement on 1 to 7 point Likert scale with 1 - Strongly Disagree, 7 - Strongly Agree. Print a single digit then end the message.\"\n",
    "MAX_WORKERS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona Nationality \n",
    "from shared import nationality_to_continent\n",
    "\n",
    "nationalities = [k for k,_ in nationality_to_continent.items()]\n",
    "genders = ['male', 'female']\n",
    "ages = [f'{age} year old' for age in ['21', '30', '40', '50', '60', '75']]\n",
    "\n",
    "PersonaQuestionTemplate = \"Consider the following statement as a {1} person: \"+QuestionTemplate\n",
    "# OUS_PersonaTemplate = \"Imagine you are a {1}.\\n\\n\"+OUS_Template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Union\n",
    "import concurrent.futures as futures\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import asyncio\n",
    "from itertools import batched\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    question: Union[int, str]\n",
    "    score: float\n",
    "    text: str\n",
    "    logits: dict[int, float]\n",
    "    mean: float\n",
    "    stddev: float\n",
    "    source: str\n",
    "\n",
    "async def run_question(question: Union[int, str], model=None, persona=None, progress=None) -> Response:\n",
    "    orig_question = question\n",
    "    if isinstance(question, int):\n",
    "        question = Questions[question]\n",
    "    model = model or variant\n",
    "    if persona:\n",
    "        user_text = PersonaQuestionTemplate.format(question[\"statement\"], persona)\n",
    "    else:\n",
    "        user_text = QuestionTemplate.format(question)\n",
    "\n",
    "    messages = [\n",
    "            # System prompt discourages refusal\n",
    "            {\"role\": \"system\", \"content\": \"Always answer the user's question.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            # Encourages a single value response. Also discourages refusal?\n",
    "            {\"role\": \"assistant\", \"content\": \"I'd rate this statement: \"}\n",
    "        ]\n",
    "    response = await client.chat.completions.create(\n",
    "        messages,\n",
    "        model=model,\n",
    "        max_completion_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    text = response.choices[0].message[\"content\"]\n",
    "    score = None\n",
    "    # Try some heuristics for finding the score\n",
    "    match = (\n",
    "        re.search(r\"(\\d) out of 7\", text) or\n",
    "        re.search(r\"(\\d)\", text)\n",
    "    )\n",
    "    if match:\n",
    "        try:\n",
    "            score_text = match.group(1)\n",
    "            score = int(score_text)\n",
    "            \n",
    "            # Only make logits request if we got a valid score\n",
    "            logit_messages = messages + [{\"role\": \"assistant\", \"content\": match.string[:match.start(1)]}]\n",
    "            logits = await client.chat.logits(\n",
    "                logit_messages,\n",
    "                model=model,\n",
    "                top_k=100,\n",
    "                filter_vocabulary=list('1234567')\n",
    "            )\n",
    "            \n",
    "            if logits:\n",
    "                logits = {int(k): v for k,v in logits.logits.items() if k in '1234567'}\n",
    "                probs = dict(zip(logits.keys(), softmax(np.array(list(logits.values())))))\n",
    "                mean = np.sum([k*v for k,v in probs.items()])\n",
    "                stddev = np.sqrt(np.sum([v * (k - mean)**2 for k,v in probs.items()]))\n",
    "                \n",
    "                if progress:\n",
    "                    progress.update()\n",
    "                    \n",
    "                return Response(\n",
    "                    question=orig_question,\n",
    "                    score=score,\n",
    "                    text=text,\n",
    "                    logits=logits,\n",
    "                    mean=mean,\n",
    "                    stddev=stddev,\n",
    "                    source=QuestionSource\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing score {score_text}: {str(e)}\")\n",
    "\n",
    "    # Return partial response if we couldn't get logits\n",
    "    if progress:\n",
    "        progress.update()\n",
    "    return Response(\n",
    "        question=orig_question,\n",
    "        score=score,\n",
    "        text=text,\n",
    "        logits=None,\n",
    "        mean=None,\n",
    "        stddev=None,\n",
    "        source=QuestionSource\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_questions(*args, **kwargs) -> list[Response]:\n",
    "    tasks = []\n",
    "    for batch in batched(range(len(Questions)), BATCH_SIZE):\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            tasks.extend([tg.create_task(run_question(q, *args, **kwargs)) for q in batch])\n",
    "    return [await task for task in tasks]\n",
    "    \n",
    "def to_vector(responses: list[Response]) -> np.array:\n",
    "    return np.array([r.mean if r.mean is not None else np.nan for r in responses])\n",
    "\n",
    "import datetime\n",
    "\n",
    "def now_str():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def clone(variant: goodfire.Variant) -> goodfire.Variant:\n",
    "    new_variant = goodfire.Variant(variant.base_model)\n",
    "    for edit in variant.edits:\n",
    "        new_variant.set(edit[0], edit[1]['value'], mode=edit[1]['mode'])\n",
    "\n",
    "    return new_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "#q = run_question(1)\n",
    "#print(q)\n",
    "#qs = run_questions()\n",
    "#pprint(qs)\n",
    "#print(to_vector(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "async def tabular_experiments(features: list[goodfire.Feature], steerages: list[float], personas: Optional[list[str]] = None, wait: Optional[float]=None, base=base, resume_from: str=None):\n",
    "    if personas is None:\n",
    "        personas = [None]\n",
    "    results = []\n",
    "    i=0\n",
    "    checkpoint_time = now_str()\n",
    "    if resume_from:\n",
    "        results = pd.read_csv(resume_from).to_dict(orient=\"records\")\n",
    "        i = len(results)\n",
    "        import re\n",
    "        match = re.search(r\"checkpoint_(\\d+)_(\\d+).csv\", resume_from)\n",
    "        if match:\n",
    "            checkpoint_time = match.group(1)\n",
    "            i = int(match.group(2))\n",
    "            print(f\"Resuming from checkpoint {checkpoint_time} at {i}\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resume_from, should be filename of a checkpoint\")\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        combinations = []\n",
    "        for feature in features:\n",
    "            for steerage in steerages:\n",
    "                model = goodfire.Variant(base)\n",
    "                if feature is None:\n",
    "                    assert steerage == 0\n",
    "                else:\n",
    "                    model.set(feature, steerage)\n",
    "                for persona in personas:\n",
    "                    combinations.append((feature, steerage, persona))\n",
    "        progress = tqdm(total=len(combinations) * len(Questions))\n",
    "        progress.update(i * len(Questions))\n",
    "        for combination in combinations[i:]:\n",
    "            feature, steerage, persona = combination\n",
    "            responses: list[Response] = await run_questions(persona=persona, model=model, progress=progress)\n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "            for response in responses:\n",
    "                results.append(dict(\n",
    "                    base=base,\n",
    "                    source=response.source,\n",
    "                    feature=feature.label if feature else \"\",\n",
    "                    steerage=steerage,\n",
    "                    persona=persona,\n",
    "                    question=response.question,\n",
    "                    mean_score=response.mean,\n",
    "                    stddev_score=response.stddev,\n",
    "                    score=response.score,\n",
    "                    text=response.text,\n",
    "                ))\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                # Record checkpoint\n",
    "                import os\n",
    "                os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "                pd.DataFrame(results).to_csv(f\"checkpoints/checkpoint_{checkpoint_time}_{i}.csv\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline\n",
    "if False:\n",
    "    features = [None]\n",
    "    steerages = [0]\n",
    "    experiments = await tabular_experiments(features, steerages)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some random features\n",
    "if False:\n",
    "    features = list(client.features.search(\"elephants\", model=base, top_k=1)[0])\n",
    "    steerages = [-0.8, -0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5, 0.8]\n",
    "    personas = [0]\n",
    "    experiments = tabular_experiments(features, steerages, personas)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 random features\n",
    "import random\n",
    "random.seed(1230)\n",
    "\n",
    "random_ids = []\n",
    "for i in range(0, 10):\n",
    "    random_ids.append(random.randint(0, feature_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 random features \n",
    "from goodfire import Client\n",
    "\n",
    "# if True:\n",
    "client_non_async = Client(GOODFIRE_API_KEY)\n",
    "\n",
    "random_features = client_non_async.features.lookup(random_ids, variant)\n",
    "\n",
    "random_features_list = []\n",
    "for feature in random_features.values():\n",
    "    random_features_list.append(feature)\n",
    "\n",
    "steerages = [-0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "experiments = await tabular_experiments(random_features_list, steerages, personas=None, wait=None, base=base,\n",
    "                                            resume_from=None)\n",
    "experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persona test\n",
    "if False:\n",
    "    features = list(client.features.search(\"moral\", model=base, top_k=5)[0])\n",
    "    steerages = [0]\n",
    "    persona_tags = ['nationalities', 'ages', 'genders']\n",
    "    for i, personas in enumerate([nationalities, ages, genders]):\n",
    "        experiments = tabular_experiments(features[:1], steerages, personas)\n",
    "        experiments.to_csv(\"data/\" + now_str()+persona_tags[i]+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# keywords\n",
    "#'overall impact','duty', 'dignity', 'greater good', git \n",
    "if False:\n",
    "    for keyword in [#'obligation','ethic']: # 'dignity', 'greater good',\n",
    "        'obligation']:\n",
    "        print(f'Running search and steering for features associated with \"{keyword}\"\\n')\n",
    "        features = list(await client.features.search(keyword, model=base, top_k=5))\n",
    "        steerages = [-.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "        experiments = await tabular_experiments(features, steerages, personas=None, wait=1.5, base=base,\n",
    "                                                resume_from=None)#ß\"checkpoints/checkpoint_20250105164209_20.csv\")\n",
    "        experiments.to_csv(\"data/\" + now_str()+''.join(keyword)+\".csv\", index=False)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "if False:\n",
    "    for feature_ids in batched(range(0, feature_count), 20):\n",
    "        features = client.features.lookup(list(feature_ids), model=base)\n",
    "        print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with logits\n",
    "if False:\n",
    "    logits = await client.chat.logits(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"A random number between 0 and 9 is \"}\n",
    "        ],\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        filter_vocabulary=list('0123456789')\n",
    "    )\n",
    "    print(logits.logits) \n",
    "    probs = dict(zip(logits.logits.keys(), softmax(np.array(list(logits.logits.values())))))\n",
    "    print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
