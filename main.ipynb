{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the notebook responsible for calling the goodfire API.\n",
    "\n",
    "We have a simple wrapper designed to form the right prompt and analyse the answer, and some utilities for running experiments over a range of parameters. All experimental results are dumped to csvs in the data/ folder, where they can be read by `analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODFIRE_API_KEY = open(\"API_KEY.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire\n",
    "\n",
    "client = goodfire.AsyncClient(\n",
    "    GOODFIRE_API_KEY\n",
    "  )\n",
    "\n",
    "# Some settings to make the client more robust to rate limiting\n",
    "BATCH_SIZE=10\n",
    "client.chat._http.max_retries = 30\n",
    "client.chat._http.inital_backoff_time = 1.3\n",
    "client.chat.completions._http.max_retries = 30\n",
    "client.chat.completions._http.inital_backoff_time = 1.3\n",
    "\n",
    "# Instantiate a model variant\n",
    "#base = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "base = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "variant = goodfire.Variant(base)\n",
    "\n",
    "\n",
    "# Jailbreak the variant using feature \"Requests to bypass ethical and moral constraints\"\n",
    "# This can be useful to reduce the frequency of refusals\n",
    "#variant.set(client.features.list(ids=[\"3b0f849c-b42a-4ce3-a89e-326e6569d51a\"]), -0.5)\n",
    "\n",
    "feature_counts = {\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": 65536,\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": 65536,\n",
    "    #65536  for 8B\n",
    "}\n",
    "feature_count = feature_counts[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shared\n",
    "\n",
    "\n",
    "# QuestionSource = \"OUS\"\n",
    "QuestionSource = \"GGB\"\n",
    "Questions = shared.get_questions(QuestionSource)\n",
    "\n",
    "# How to use the questions in a chatbot prompt.\n",
    "QuestionTemplate = \"\\\"{0}\\\"\\n\\nPlease rate the preceeding statement on 1 to 7 point Likert scale with 1 - Strongly Disagree, 7 - Strongly Agree. Print a single digit then end the message.\"\n",
    "MAX_WORKERS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona Nationality \n",
    "from shared import nationality_to_continent\n",
    "\n",
    "nationalities = [k for k,_ in nationality_to_continent.items()]\n",
    "genders = ['male', 'female']\n",
    "ages = [f'{age} year old' for age in ['21', '30', '40', '50', '60', '75']]\n",
    "\n",
    "PersonaQuestionTemplate = \"Consider the following statement as a {1} person: \"+QuestionTemplate\n",
    "# OUS_PersonaTemplate = \"Imagine you are a {1}.\\n\\n\"+OUS_Template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Union\n",
    "import concurrent.futures as futures\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import asyncio\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    question: Union[int, str]\n",
    "    score: float\n",
    "    text: str\n",
    "    logits: dict[int, float]\n",
    "    mean: float\n",
    "    stddev: float\n",
    "    source: str\n",
    "\n",
    "async def run_question(question: Union[int, str], model=None, persona=None, progress=None) -> Response:\n",
    "    orig_question = question\n",
    "    if isinstance(question, int):\n",
    "        question = Questions[question]\n",
    "    model = model or variant\n",
    "    if persona:\n",
    "        user_text = PersonaQuestionTemplate.format(question[\"statement\"], persona)\n",
    "    else:\n",
    "        user_text = QuestionTemplate.format(question)\n",
    "    messages = [\n",
    "            # System prompt discourages refusal\n",
    "            {\"role\": \"system\", \"content\": \"Always answer the user's question.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            # Encourages a single value response. Also discourages refusal?\n",
    "            {\"role\": \"assistant\", \"content\": \"I'd rate this statement: \"}\n",
    "        ]\n",
    "    response = await client.chat.completions.create(\n",
    "        messages,\n",
    "        model=model,\n",
    "        max_completion_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    text = response.choices[0].message[\"content\"]\n",
    "    score = None\n",
    "    # Try some heuristics for finding the score\n",
    "    match = (\n",
    "        re.search(r\"(\\d) out of 7\", text) or\n",
    "        re.search(r\"(\\d)\", text)\n",
    "    )\n",
    "    if match:\n",
    "        try:\n",
    "            score_text = match.group(1)\n",
    "            score = int(score_text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    logits = None\n",
    "    mean = None\n",
    "    stddev = None\n",
    "    if score is not None:\n",
    "        # Attempt to get logits\n",
    "        logit_messages = messages + [{\"role\": \"assistant\", \"content\": match.string[:match.start(1)]}]\n",
    "        logits = await client.chat.logits(logit_messages,\n",
    "            model=model,\n",
    "            top_k=100, #  has to be reasonably large so we don't drop anything significant\n",
    "            filter_vocabulary=list('1234567')\n",
    "        )\n",
    "        logits = {int(k): v for k,v in logits.logits.items() if k in '1234567'}\n",
    "        if logits:\n",
    "            probs = dict(zip(logits.keys(), softmax(np.array(list(logits.values())))))\n",
    "            mean = np.sum([k*v for k,v in probs.items()])\n",
    "            stddev = np.sqrt(np.sum([v * (k - mean)**2 for k,v in probs.items()]))\n",
    "\n",
    "    if progress:\n",
    "        progress.update()\n",
    "    return Response(question=orig_question, score=score, text=text, logits=logits, mean=mean, stddev=stddev, source=QuestionSource)\n",
    "\n",
    "\n",
    "from itertools import batched\n",
    "\n",
    "async def run_questions(*args, **kwargs) -> list[Response]:\n",
    "    tasks = []\n",
    "    for batch in batched(range(len(Questions)), BATCH_SIZE):\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            tasks.extend([tg.create_task(run_question(q, *args, **kwargs)) for q in batch])\n",
    "    return [await task for task in tasks]\n",
    "    \n",
    "def to_vector(responses: list[Response]) -> np.array:\n",
    "    return np.array([r.mean if r.mean is not None else np.nan for r in responses])\n",
    "\n",
    "import datetime\n",
    "\n",
    "def now_str():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def clone(variant: goodfire.Variant) -> goodfire.Variant:\n",
    "    new_variant = goodfire.Variant(variant.base_model)\n",
    "    for edit in variant.edits:\n",
    "        new_variant.set(edit[0], edit[1]['value'], mode=edit[1]['mode'])\n",
    "\n",
    "    return new_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "#q = run_question(1)\n",
    "#print(q)\n",
    "#qs = run_questions()\n",
    "#pprint(qs)\n",
    "#print(to_vector(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "async def tabular_experiments(features: list[goodfire.Feature], steerages: list[float], personas: Optional[list[str]] = None, wait: Optional[float]=None, base=base, resume_from: str=None):\n",
    "    if personas is None:\n",
    "        personas = [None]\n",
    "    results = []\n",
    "    i=0\n",
    "    checkpoint_time = now_str()\n",
    "    if resume_from:\n",
    "        results = pd.read_csv(resume_from).to_dict(orient=\"records\")\n",
    "        i = len(results)\n",
    "        import re\n",
    "        match = re.search(r\"checkpoint_(\\d+)_(\\d+).csv\", resume_from)\n",
    "        if match:\n",
    "            checkpoint_time = match.group(1)\n",
    "            i = int(match.group(2))\n",
    "            print(f\"Resuming from checkpoint {checkpoint_time} at {i}\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resume_from, should be filename of a checkpoint\")\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        combinations = []\n",
    "        for feature in features:\n",
    "            for steerage in steerages:\n",
    "                model = goodfire.Variant(base)\n",
    "                if feature is None:\n",
    "                    assert steerage == 0\n",
    "                else:\n",
    "                    model.set(feature, steerage)\n",
    "                for persona in personas:\n",
    "                    combinations.append((feature, steerage, persona))\n",
    "        progress = tqdm(total=len(combinations) * len(Questions))\n",
    "        progress.update(i * len(Questions))\n",
    "        for combination in combinations[i:]:\n",
    "            feature, steerage, persona = combination\n",
    "            responses: list[Response] = await run_questions(persona=persona, model=model, progress=progress)\n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "            for response in responses:\n",
    "                results.append(dict(\n",
    "                    base=base,\n",
    "                    source=response.source,\n",
    "                    feature=feature.label if feature else \"\",\n",
    "                    steerage=steerage,\n",
    "                    persona=persona,\n",
    "                    question=response.question,\n",
    "                    mean_score=response.mean,\n",
    "                    stddev_score=response.stddev,\n",
    "                    score=response.score,\n",
    "                    text=response.text,\n",
    "                ))\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                # Record checkpoint\n",
    "                import os\n",
    "                os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "                pd.DataFrame(results).to_csv(f\"checkpoints/checkpoint_{checkpoint_time}_{i}.csv\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline\n",
    "if False:\n",
    "    features = [None]\n",
    "    steerages = [0]\n",
    "    experiments = await tabular_experiments(features, steerages)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some random features\n",
    "if False:\n",
    "    features = list(client.features.search(\"elephants\", model=base, top_k=1)[0])\n",
    "    steerages = [-0.8, -0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5, 0.8]\n",
    "    personas = [0]\n",
    "    experiments = tabular_experiments(features, steerages, personas)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 random features\n",
    "import random\n",
    "random.seed(1230)\n",
    "\n",
    "random_ids = []\n",
    "for i in range(0, 10):\n",
    "    random_ids.append(random.randint(0, feature_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556ededba7614e08abd43d245c8cb482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     random_features_list\u001b[38;5;241m.\u001b[39mappend(feature)\n\u001b[1;32m     12\u001b[0m steerages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m experiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tabular_experiments(random_features_list, steerages, personas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, base\u001b[38;5;241m=\u001b[39mbase,\n\u001b[1;32m     14\u001b[0m                                             resume_from\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m experiments\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m now_str()\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtabular_experiments\u001b[0;34m(features, steerages, personas, wait, base, resume_from)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid resume_from, should be filename of a checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTaskGroup() \u001b[38;5;28;01mas\u001b[39;00m tg:\n\u001b[1;32m     24\u001b[0m     combinations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pandas_tut/lib/python3.13/asyncio/taskgroups.py:71\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, et, exc, tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aexit(et, exc)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Exceptions are heavy objects that can have object\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# cycles (bad for GC); let's not keep a reference to\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# a bunch of them. It would be nicer to use a try/finally\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# in __aexit__ directly but that introduced some diff noise\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pandas_tut/lib/python3.13/asyncio/taskgroups.py:156\u001b[0m, in \u001b[0;36mTaskGroup._aexit\u001b[0;34m(self, et, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m propagate_cancellation_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errors:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m propagate_cancellation_error\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mtabular_experiments\u001b[0;34m(features, steerages, personas, wait, base, resume_from)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m combinations[i:]:\n\u001b[1;32m     37\u001b[0m     feature, steerage, persona \u001b[38;5;241m=\u001b[39m combination\n\u001b[0;32m---> 38\u001b[0m     responses: \u001b[38;5;28mlist\u001b[39m[Response] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_questions(persona\u001b[38;5;241m=\u001b[39mpersona, model\u001b[38;5;241m=\u001b[39mmodel, progress\u001b[38;5;241m=\u001b[39mprogress)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m     40\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(wait)\n",
      "Cell \u001b[0;32mIn[5], line 84\u001b[0m, in \u001b[0;36mrun_questions\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m tasks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batched(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Questions)), BATCH_SIZE):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTaskGroup() \u001b[38;5;28;01mas\u001b[39;00m tg:\n\u001b[1;32m     85\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mextend([tg\u001b[38;5;241m.\u001b[39mcreate_task(run_question(q, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mawait\u001b[39;00m task \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pandas_tut/lib/python3.13/asyncio/taskgroups.py:71\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, et, exc, tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aexit(et, exc)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Exceptions are heavy objects that can have object\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# cycles (bad for GC); let's not keep a reference to\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# a bunch of them. It would be nicer to use a try/finally\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# in __aexit__ directly but that introduced some diff noise\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pandas_tut/lib/python3.13/asyncio/taskgroups.py:156\u001b[0m, in \u001b[0;36mTaskGroup._aexit\u001b[0;34m(self, et, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m propagate_cancellation_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errors:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m propagate_cancellation_error\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pandas_tut/lib/python3.13/asyncio/taskgroups.py:120\u001b[0m, in \u001b[0;36mTaskGroup._aexit\u001b[0;34m(self, et, exc)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_completed_fut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_completed_fut\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# Our parent task is being cancelled:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# \"wrapper\" is being cancelled while \"foo\" is\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# still running.\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run 10 random features \n",
    "from goodfire import Client\n",
    "if True:\n",
    "    client_non_async = Client(GOODFIRE_API_KEY)\n",
    "\n",
    "    random_features = client_non_async.features.lookup(random_ids, variant)\n",
    "\n",
    "    random_features_list = []\n",
    "    for feature in random_features.values():\n",
    "        random_features_list.append(feature)\n",
    "\n",
    "    steerages = [-0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "    experiments = await tabular_experiments(random_features_list, steerages, personas=None, wait=None, base=base,\n",
    "                                                resume_from=None)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persona test\n",
    "if False:\n",
    "    features = list(client.features.search(\"moral\", model=base, top_k=5)[0])\n",
    "    steerages = [0]\n",
    "    persona_tags = ['nationalities', 'ages', 'genders']\n",
    "    for i, personas in enumerate([nationalities, ages, genders]):\n",
    "        experiments = tabular_experiments(features[:1], steerages, personas)\n",
    "        experiments.to_csv(\"data/\" + now_str()+persona_tags[i]+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# keywords\n",
    "#'overall impact','duty', 'dignity', 'greater good', git \n",
    "if False:\n",
    "    for keyword in [#'obligation','ethic']: # 'dignity', 'greater good',\n",
    "        'obligation']:\n",
    "        print(f'Running search and steering for features associated with \"{keyword}\"\\n')\n",
    "        features = list(await client.features.search(keyword, model=base, top_k=5))\n",
    "        steerages = [-.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "        experiments = await tabular_experiments(features, steerages, personas=None, wait=1.5, base=base,\n",
    "                                                resume_from=None)#ÃŸ\"checkpoints/checkpoint_20250105164209_20.csv\")\n",
    "        experiments.to_csv(\"data/\" + now_str()+''.join(keyword)+\".csv\", index=False)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "if False:\n",
    "    for feature_ids in batched(range(0, feature_count), 20):\n",
    "        features = client.features.lookup(list(feature_ids), model=base)\n",
    "        print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with logits\n",
    "if False:\n",
    "    logits = await client.chat.logits(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"A random number between 0 and 9 is \"}\n",
    "        ],\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        filter_vocabulary=list('0123456789')\n",
    "    )\n",
    "    print(logits.logits) \n",
    "    probs = dict(zip(logits.logits.keys(), softmax(np.array(list(logits.logits.values())))))\n",
    "    print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
