{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the notebook responsible for calling the goodfire API.\n",
    "\n",
    "We have a simple wrapper designed to form the right prompt and analyse the answer, and some utilities for running experiments over a range of parameters. All experimental results are dumped to csvs in the data/ folder, where they can be read by `analysis.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODFIRE_API_KEY = open(\"API_KEY.txt\", \"r\").read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire\n",
    "\n",
    "client = goodfire.AsyncClient(\n",
    "    GOODFIRE_API_KEY\n",
    "  )\n",
    "\n",
    "# Some settings to make the client more robust to rate limiting\n",
    "BATCH_SIZE=10\n",
    "client.chat._http.max_retries = 30\n",
    "client.chat._http.inital_backoff_time = 1.3\n",
    "client.chat.completions._http.max_retries = 30\n",
    "client.chat.completions._http.inital_backoff_time = 1.3\n",
    "\n",
    "# Instantiate a model variant\n",
    "#base = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "base = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "#base = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "variant = goodfire.Variant(base)\n",
    "\n",
    "\n",
    "# Jailbreak the variant using feature \"Requests to bypass ethical and moral constraints\"\n",
    "# This can be useful to reduce the frequency of refusals\n",
    "#variant.set(client.features.list(ids=[\"3b0f849c-b42a-4ce3-a89e-326e6569d51a\"]), -0.5)\n",
    "\n",
    "feature_counts = {\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": 65536,\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": 65536,\n",
    "    #65536  for 8B\n",
    "}\n",
    "feature_count = feature_counts[base]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEYWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the keywords to be reproducible. Here is the prompt: <br>\n",
    "\n",
    "\n",
    "**What are the top 5 keywords to identify moral values in language models, focusing on utilitarianism? Only suggest words that are specifically moral/ethical in nature, avoiding both technical philosophy terms (e.g., 'Satisfaction of Preferences') and general terms that commonly appear in non-moral contexts (e.g., 'better', 'maximize', 'good', 'bad', 'right', 'values'). Please verify that the terms you are choosing are neither too technical nor too broad before responding. Respond with only the keywords**\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the responses of 4 language models: <br>\n",
    "\n",
    "** Llama-3.3-70B - Goodfire **\n",
    "Altruism, Empathy, Fairness, Compassion, Benevolence\n",
    "\n",
    "** Llama-3.1-8B - Goodfire ** \n",
    "1. Justice\n",
    "2. Fairness\n",
    "3. Harm\n",
    "4. Suffering\n",
    "5. Wellbeing\n",
    "\n",
    "** Chat-GPT 4o mini ** \n",
    "Well-being\n",
    "Happiness\n",
    "Consequences\n",
    "Harm\n",
    "Utility\n",
    "\n",
    "** Claude 3.5 Sonnet - Coincise **\n",
    "ethical\n",
    "virtuous\n",
    "benevolent\n",
    "righteous\n",
    "altruistic\n",
    "\n",
    "\n",
    "### NOTE THAT WORDS THAT ARE NOT ON THIS LIST, but that we've already RUN:\n",
    "'moral', 'greater good', 'ethic' - though 'ethical' is, 'integrity', 'dignity'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords - what features have we not yet run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#the keywords we picked previously\n",
    "# previous_keywords = ['moral', 'altruism', 'greater good', 'ethic', 'integrity', 'dignity']\n",
    "keywords = ['moral'] #, 'altruism', 'greater good', 'ethic', 'integrity', 'dignity']\n",
    "\n",
    "#they keywords based on top 5 keywords suggested by LLMs\n",
    "# keywords = ['altruism', 'benevolence', 'compassion', 'ethical', 'fairness', 'happiness', 'harm', 'justice', 'righteous', 'suffering', 'utility', 'virtuous', 'wellbeing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_features_all = []\n",
    "\n",
    "# for word in previous_keywords:\n",
    "#     previous_features_all.append(list((await client.features.search(word, model=base, top_k=10))))\n",
    "\n",
    "# flat_list = list(chain(*previous_features_all))\n",
    "# previous_features = list(set(flat_list))\n",
    "\n",
    "# n_redundant = len(flat_list) - len(previous_features)\n",
    "# if n_redundant != 0 :\n",
    "#     print(f'there are {n_redundant} features that are redundant in previous features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = []\n",
    "for word in keywords:\n",
    "    features_all.append(list((await client.features.search(word, model=base, top_k=5))))\n",
    "\n",
    "flat_list = list(chain(*features_all))\n",
    "features = list(set(flat_list))\n",
    "\n",
    "n_redundant = len(flat_list) - len(features)\n",
    "if n_redundant != 0 :\n",
    "    print(f'there are {n_redundant} features that are redundant amongst llm features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_run = list(set(features))\n",
    "print(f'Features to run: {len(features_to_run)} out of {len(features)}')\n",
    "for f in features_to_run:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shared\n",
    "\n",
    "\n",
    "QuestionSource = \"OUS\"\n",
    "# QuestionSource = \"GGB\"\n",
    "Questions = shared.get_questions(QuestionSource)\n",
    "\n",
    "# How to use the questions in a chatbot prompt.\n",
    "QuestionTemplate = \"\\\"{0}\\\"\\n\\nPlease rate the preceeding statement on 1 to 7 point Likert scale with 1 - Strongly Disagree, 7 - Strongly Agree. Print a single digit then end the message.\"\n",
    "MAX_WORKERS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Persona Nationality \n",
    "# from shared import nationality_to_continent\n",
    "\n",
    "# nationalities = [k for k,_ in nationality_to_continent.items()]\n",
    "# genders = ['male', 'female']\n",
    "# ages = [f'{age} year old' for age in ['21', '30', '40', '50', '60', '75']]\n",
    "\n",
    "# PersonaQuestionTemplate = \"Consider the following statement as a {1} person: \"+QuestionTemplate\n",
    "# # OUS_PersonaTemplate = \"Imagine you are a {1}.\\n\\n\"+OUS_Template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "from typing import Union\n",
    "import concurrent.futures as futures\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import asyncio\n",
    "from itertools import batched\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    question: Union[int, str]\n",
    "    score: float\n",
    "    text: str\n",
    "    logits: dict[int, float]\n",
    "    mean: float\n",
    "    stddev: float\n",
    "    source: str\n",
    "\n",
    "async def run_question(question: Union[int, str], model=None, persona=None, progress=None) -> Response:\n",
    "    orig_question = question\n",
    "    if isinstance(question, int):\n",
    "        question = Questions[question]\n",
    "    model = model or variant\n",
    "    if persona:\n",
    "        user_text = PersonaQuestionTemplate.format(question[\"statement\"], persona)\n",
    "    else:\n",
    "        user_text = QuestionTemplate.format(question)\n",
    "\n",
    "    messages = [\n",
    "            # System prompt discourages refusal\n",
    "            {\"role\": \"system\", \"content\": \"Always answer the user's question.\"},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "            # Encourages a single value response. Also discourages refusal?\n",
    "            {\"role\": \"assistant\", \"content\": \"I'd rate this statement: \"}\n",
    "        ]\n",
    "    response = await client.chat.completions.create(\n",
    "        messages,\n",
    "        model=model,\n",
    "        max_completion_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    text = response.choices[0].message[\"content\"]\n",
    "    score = None\n",
    "    # Try some heuristics for finding the score\n",
    "    match = (\n",
    "        re.search(r\"(\\d) out of 7\", text) or\n",
    "        re.search(r\"(\\d)\", text)\n",
    "    )\n",
    "    if match:\n",
    "        try:\n",
    "            score_text = match.group(1)\n",
    "            score = int(score_text)\n",
    "            \n",
    "            # Only make logits request if we got a valid score\n",
    "            logit_messages = messages + [{\"role\": \"assistant\", \"content\": match.string[:match.start(1)]}]\n",
    "            logits = await client.chat.logits(\n",
    "                logit_messages,\n",
    "                model=model,\n",
    "                top_k=100,\n",
    "                filter_vocabulary=list('1234567')\n",
    "            )\n",
    "            \n",
    "            if logits:\n",
    "                logits = {int(k): v for k,v in logits.logits.items() if k in '1234567'}\n",
    "                probs = dict(zip(logits.keys(), softmax(np.array(list(logits.values())))))\n",
    "                mean = np.sum([k*v for k,v in probs.items()])\n",
    "                stddev = np.sqrt(np.sum([v * (k - mean)**2 for k,v in probs.items()]))\n",
    "                \n",
    "                if progress:\n",
    "                    progress.update()\n",
    "                    \n",
    "                return Response(\n",
    "                    question=orig_question,\n",
    "                    score=score,\n",
    "                    text=text,\n",
    "                    logits=logits,\n",
    "                    mean=mean,\n",
    "                    stddev=stddev,\n",
    "                    source=QuestionSource\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing score {score_text}: {str(e)}\")\n",
    "\n",
    "    # Return partial response if we couldn't get logits\n",
    "    if progress:\n",
    "        progress.update()\n",
    "    return Response(\n",
    "        question=orig_question,\n",
    "        score=score,\n",
    "        text=text,\n",
    "        logits=None,\n",
    "        mean=None,\n",
    "        stddev=None,\n",
    "        source=QuestionSource\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_questions(*args, progress=None, completed_qs=set(), **kwargs) -> list[Response]:\n",
    "    tasks = []\n",
    "    for batch in batched(range(len(Questions)), BATCH_SIZE):\n",
    "        async with asyncio.TaskGroup() as tg:\n",
    "            for q in batch:\n",
    "                # Simply skip any question that has already been run\n",
    "                if q in completed_qs:\n",
    "                    if progress: progress.update()\n",
    "                    continue\n",
    "                tasks.append(tg.create_task(run_question(q, *args, progress=progress, **kwargs)))\n",
    "    return [await task for task in tasks]\n",
    "\n",
    "def to_vector(responses: list[Response]) -> np.array:\n",
    "    return np.array([r.mean if r.mean is not None else np.nan for r in responses])\n",
    "\n",
    "import datetime\n",
    "\n",
    "def now_str():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def clone(variant: goodfire.Variant) -> goodfire.Variant:\n",
    "    new_variant = goodfire.Variant(variant.base_model)\n",
    "    for edit in variant.edits:\n",
    "        new_variant.set(edit[0], edit[1]['value'], mode=edit[1]['mode'])\n",
    "\n",
    "    return new_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "#q = run_question(1)\n",
    "#print(q)\n",
    "#qs = run_questions()\n",
    "#pprint(qs)\n",
    "#print(to_vector(qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "async def tabular_experiments(features: list[goodfire.Feature], steerages: list[float], personas: Optional[list[str]] = None, wait: Optional[float]=None, base=base, resume_from: str=None, skip_completed=None):\n",
    "    if personas is None:\n",
    "        personas = [None]\n",
    "    results = []\n",
    "    i=0\n",
    "    checkpoint_time = now_str()\n",
    "    if resume_from:\n",
    "        results = pd.read_csv(resume_from).to_dict(orient=\"records\")\n",
    "        i = len(results)\n",
    "        import re\n",
    "        match = re.search(r\"checkpoint_(\\d+)_(\\d+).csv\", resume_from)\n",
    "        if match:\n",
    "            checkpoint_time = match.group(1)\n",
    "            i = int(match.group(2))\n",
    "            print(f\"Resuming from checkpoint {checkpoint_time} at {i}\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid resume_from, should be filename of a checkpoint\")\n",
    "    async with asyncio.TaskGroup() as tg:\n",
    "        combinations = []\n",
    "        for feature in features:\n",
    "            for steerage in steerages:\n",
    "                model = goodfire.Variant(base)\n",
    "                if feature is None:\n",
    "                    assert steerage == 0\n",
    "                else:\n",
    "                    model.set(feature, steerage)\n",
    "                for persona in personas:\n",
    "                    combinations.append((feature, steerage, persona))\n",
    "        completed_dict = {}\n",
    "        if skip_completed:\n",
    "            completed_df: pd.DataFrame = pd.concat([pd.read_csv(c) for c in skip_completed])\n",
    "            completed_df = completed_df[completed_df[\"text\"].apply(lambda t: not t.startswith(\"Failed\"))]\n",
    "            completed_df.fillna({'feature': '', 'persona': ''}, inplace=True)\n",
    "            completed_dict = completed_df.groupby([\"feature\", \"steerage\", \"persona\"])[\"question\"].apply(set)\n",
    "        progress = tqdm(total=len(combinations) * len(Questions))\n",
    "        progress.update(i * len(Questions))\n",
    "        for combination in combinations[i:]:\n",
    "            feature, steerage, persona = combination\n",
    "            feature_label = feature.label if feature else \"\"\n",
    "            persona_label = persona or \"\"\n",
    "            completed_qs = completed_dict.get((feature_label, steerage, persona_label), [])\n",
    "            responses: list[Response] = await run_questions(persona=persona, model=model, progress=progress, completed_qs=completed_qs)\n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "            for response in responses:\n",
    "                results.append(dict(\n",
    "                    base=base,\n",
    "                    source=response.source,\n",
    "                    feature=feature_label,\n",
    "                    steerage=steerage,\n",
    "                    persona=persona,\n",
    "                    question=response.question,\n",
    "                    mean_score=response.mean,\n",
    "                    stddev_score=response.stddev,\n",
    "                    score=response.score,\n",
    "                    text=response.text,\n",
    "                ))\n",
    "            i += 1\n",
    "            if i % 10 == 0:\n",
    "                # Record checkpoint\n",
    "                import os\n",
    "                os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "                pd.DataFrame(results).to_csv(f\"checkpoints/checkpoint_{checkpoint_time}_{i}.csv\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline without features\n",
    "if False:\n",
    "    features = [None]\n",
    "    steerages = [0]\n",
    "    experiments = await tabular_experiments(features, steerages)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline with top 5 features\n",
    "if False:\n",
    "    steerages = [0]\n",
    "    experiments = await tabular_experiments(features_to_run, steerages)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline with top 5 features + steerages\n",
    "if True:\n",
    "    steerages = [-0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "    experiments = await tabular_experiments(features_to_run, steerages)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some random features\n",
    "if False:\n",
    "    features = list(client.features.search(\"elephants\", model=base, top_k=1)[0])\n",
    "    steerages = [-0.8, -0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5, 0.8]\n",
    "    personas = [0]\n",
    "    experiments = tabular_experiments(features, steerages, personas)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 random features\n",
    "if False:\n",
    "    import random\n",
    "    random.seed(1230)\n",
    "\n",
    "    random_ids = []\n",
    "    for i in range(0, 10):\n",
    "        random_ids.append(random.randint(0, feature_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 random features \n",
    "from goodfire import Client\n",
    "\n",
    "if False:\n",
    "    client_non_async = Client(GOODFIRE_API_KEY)\n",
    "\n",
    "    random_features = client_non_async.features.lookup(random_ids, variant)\n",
    "\n",
    "    random_features_list = []\n",
    "    for feature in random_features.values():\n",
    "        random_features_list.append(feature)\n",
    "\n",
    "    steerages = [-0.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "    experiments = await tabular_experiments(random_features_list, steerages, personas=None, wait=None, base=base,\n",
    "                                                resume_from=None)\n",
    "    experiments.to_csv(\"data/\" + now_str()+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persona test\n",
    "if False:\n",
    "    features = list(client.features.search(\"moral\", model=base, top_k=5)[0])\n",
    "    steerages = [0]\n",
    "    persona_tags = ['nationalities', 'ages', 'genders']\n",
    "    for i, personas in enumerate([nationalities, ages, genders]):\n",
    "        experiments = tabular_experiments(features[:1], steerages, personas)\n",
    "        experiments.to_csv(\"data/\" + now_str()+persona_tags[i]+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# keywords\n",
    "#'overall impact','duty', 'dignity', 'greater good', git \n",
    "if False:\n",
    "    for keyword in [#'obligation','ethic']: # 'dignity', 'greater good',\n",
    "        'dignity']:\n",
    "        print(f'Running search and steering for features associated with \"{keyword}\"\\n')\n",
    "        features = list(await client.features.search(keyword, model=base, top_k=5))\n",
    "        steerages = [-.5, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.5]\n",
    "        experiments = await tabular_experiments(features, steerages, personas=None, wait=1.5, base=base,\n",
    "                                                resume_from=None,#\"checkpoints/checkpoint_20250105164209_20.csv\")\n",
    "                                                skip_completed=None #skip_completed=[\"data/20250104091128_dignity.csv\"]\n",
    "                                                )\n",
    "        experiments.to_csv(\"data/\" + now_str()+''.join(keyword)+\".csv\", index=False)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "if False:\n",
    "    for feature_ids in batched(range(0, feature_count), 20):\n",
    "        features = client.features.lookup(list(feature_ids), model=base)\n",
    "        print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with logits\n",
    "if False:\n",
    "    logits = await client.chat.logits(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"A random number between 0 and 9 is \"}\n",
    "        ],\n",
    "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        filter_vocabulary=list('0123456789')\n",
    "    )\n",
    "    print(logits.logits) \n",
    "    probs = dict(zip(logits.logits.keys(), softmax(np.array(list(logits.logits.values())))))\n",
    "    print(probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_tut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
